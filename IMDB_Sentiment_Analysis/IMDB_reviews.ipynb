{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "nominated-notice",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Must libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "composite-operation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "imdb_data=pd.read_csv('../../data/IMDB_reviews.csv')\n",
    "\n",
    "# print(imdb_data.shape,'\\n')\n",
    "# print(imdb_data.head(10),'\\n')\n",
    "# print(imdb_data['sentiment'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unexpected-glucose",
   "metadata": {},
   "source": [
    "### Step 1 - Preprocessing Text to tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "painted-response",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "convenient-worthy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 11 Âµs\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "# Pre-processing text data\n",
    "\n",
    "def preprocessing(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    text = soup.get_text()     # Step 1 - Remove html tags\n",
    "    pattern = r'[^a-zA-z0-9\\s]' \n",
    "    text = re.sub(pattern,'',text)   # Step 2 - Remove punctuation\n",
    "    text = text.lower()     # Step 3 - Make text lowercase\n",
    "    tokenizer=ToktokTokenizer()\n",
    "    tokens = tokenizer.tokenize(text)    # Step 4 - Tokenizing\n",
    "    lemma = WordNetLemmatizer()          # Step 5 - Lemmatizing\n",
    "    stopword_list=nltk.corpus.stopwords.words('english')\n",
    "    stop=set(stopwords.words('english'))\n",
    "    filtered_tokens = [lemma.lemmatize(word).strip() for word in tokens if word not in stopword_list]    # Step 6 - Removing stopwords\n",
    "    text = TreebankWordDetokenizer().detokenize(filtered_tokens)\n",
    "    return text\n",
    "\n",
    "#Apply function on review column\n",
    "imdb_data['review'] = imdb_data['review'].apply(preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "adjustable-apparatus",
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_data.to_csv('IMDB_reviews_preprocessed.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sublime-decimal",
   "metadata": {},
   "source": [
    "### Step 2 - Tokens to features and train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "vertical-instrumentation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>one reviewer mentioned watching 1 oz episode y...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>wonderful little production filming technique ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>thought wonderful way spend time hot summer we...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>basically there family little boy jake think t...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>petter matteis love time money visually stunni...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  one reviewer mentioned watching 1 oz episode y...  positive\n",
       "1  wonderful little production filming technique ...  positive\n",
       "2  thought wonderful way spend time hot summer we...  positive\n",
       "3  basically there family little boy jake think t...  negative\n",
       "4  petter matteis love time money visually stunni...  positive"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_data = pd.read_csv('IMDB_reviews_preprocessed.csv')\n",
    "imdb_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "critical-fiction",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_train_reviews=imdb_data.review[:40000]\n",
    "norm_test_reviews=imdb_data.review[40000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bronze-focus",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.linear_model import LogisticRegression,SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n",
    "\n",
    "def dummy(doc):\n",
    "    return doc\n",
    "\n",
    "cv = CountVectorizer(\n",
    "        tokenizer=dummy,\n",
    "        preprocessor=dummy\n",
    "    )  \n",
    "# #Count vectorizer for bag of words\n",
    "# cv=CountVectorizer(tokenizer=None,ngram_range=(1,3))\n",
    "#transformed train reviews\n",
    "cv_train_reviews=cv.fit_transform(norm_train_reviews)\n",
    "cv_test_reviews=cv.transform(norm_test_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "velvet-serum",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 1)\n"
     ]
    }
   ],
   "source": [
    "#labeling the sentiment data\n",
    "lb=LabelBinarizer()\n",
    "#transformed sentiment data\n",
    "sentiment_data=lb.fit_transform(imdb_data['sentiment'])\n",
    "print(sentiment_data.shape)\n",
    "#Spliting the sentiment data\n",
    "train_sentiments=sentiment_data[:40000]\n",
    "test_sentiments=sentiment_data[40000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affiliated-chicago",
   "metadata": {},
   "source": [
    "### Step 3 - Fit and predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "racial-festival",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=1, max_iter=500, random_state=42)\n"
     ]
    }
   ],
   "source": [
    "#training the model\n",
    "lr=LogisticRegression(penalty='l2',max_iter=500,C=1,random_state=42)\n",
    "#Fitting the model for Bag of words\n",
    "lr_bow=lr.fit(cv_train_reviews,train_sentiments)\n",
    "print(lr_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "representative-difficulty",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1 ... 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "#Predicting the model for bag of words\n",
    "lr_bow_predict=lr.predict(cv_test_reviews)\n",
    "print(lr_bow_predict)\n",
    "# ##Predicting the model for tfidf features\n",
    "# lr_tfidf_predict=lr.predict(tv_test_reviews)\n",
    "# print(lr_tfidf_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intensive-voltage",
   "metadata": {},
   "source": [
    "### Step 4 - Metrics Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "patent-people",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr_bow_score : 0.8851\n"
     ]
    }
   ],
   "source": [
    "#Accuracy score for bag of words\n",
    "lr_bow_score=accuracy_score(test_sentiments,lr_bow_predict)\n",
    "print(\"lr_bow_score :\",lr_bow_score)\n",
    "\n",
    "# #Accuracy score for tfidf features\n",
    "# lr_tfidf_score=accuracy_score(test_sentiments,lr_tfidf_predict)\n",
    "# print(\"lr_tfidf_score :\",lr_tfidf_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "material-passage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Positive       0.89      0.88      0.88      4993\n",
      "    Negative       0.88      0.89      0.89      5007\n",
      "\n",
      "    accuracy                           0.89     10000\n",
      "   macro avg       0.89      0.89      0.89     10000\n",
      "weighted avg       0.89      0.89      0.89     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Classification report for bag of words \n",
    "lr_bow_report=classification_report(test_sentiments,lr_bow_predict,target_names=['Positive','Negative'])\n",
    "print(lr_bow_report)\n",
    "\n",
    "# #Classification report for tfidf features\n",
    "# lr_tfidf_report=classification_report(test_sentiments,lr_tfidf_predict,target_names=['Positive','Negative'])\n",
    "# print(lr_tfidf_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "imported-apartment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4442  565]\n",
      " [ 584 4409]]\n"
     ]
    }
   ],
   "source": [
    "#confusion matrix for bag of words\n",
    "cm_bow=confusion_matrix(test_sentiments,lr_bow_predict,labels=[1,0])\n",
    "print(cm_bow)\n",
    "\n",
    "# #confusion matrix for tfidf features\n",
    "# cm_tfidf=confusion_matrix(test_sentiments,lr_tfidf_predict,labels=[1,0])\n",
    "# print(cm_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "innovative-tennessee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB()\n"
     ]
    }
   ],
   "source": [
    "#training the model\n",
    "mnb=MultinomialNB()\n",
    "#fitting the svm for bag of words\n",
    "mnb_bow=mnb.fit(cv_train_reviews,train_sentiments)\n",
    "print(mnb_bow)\n",
    "# #fitting the svm for tfidf features\n",
    "# mnb_tfidf=mnb.fit(tv_train_reviews,train_sentiments)\n",
    "# print(mnb_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "inclusive-presence",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "#Predicting the model for bag of words\n",
    "mnb_bow_predict=mnb.predict(cv_test_reviews)\n",
    "print(mnb_bow_predict)\n",
    "# #Predicting the model for tfidf features\n",
    "# mnb_tfidf_predict=mnb.predict(tv_test_reviews)\n",
    "# print(mnb_tfidf_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "stuck-language",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mnb_bow_score : 0.8568\n"
     ]
    }
   ],
   "source": [
    "#Accuracy score for bag of words\n",
    "mnb_bow_score=accuracy_score(test_sentiments,mnb_bow_predict)\n",
    "print(\"mnb_bow_score :\",mnb_bow_score)\n",
    "# #Accuracy score for tfidf features\n",
    "# mnb_tfidf_score=accuracy_score(test_sentiments,mnb_tfidf_predict)\n",
    "# print(\"mnb_tfidf_score :\",mnb_tfidf_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "comparable-board",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Positive       0.84      0.88      0.86      4993\n",
      "    Negative       0.87      0.84      0.85      5007\n",
      "\n",
      "    accuracy                           0.86     10000\n",
      "   macro avg       0.86      0.86      0.86     10000\n",
      "weighted avg       0.86      0.86      0.86     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Classification report for bag of words \n",
    "mnb_bow_report=classification_report(test_sentiments,mnb_bow_predict,target_names=['Positive','Negative'])\n",
    "print(mnb_bow_report)\n",
    "# #Classification report for tfidf features\n",
    "# mnb_tfidf_report=classification_report(test_sentiments,mnb_tfidf_predict,target_names=['Positive','Negative'])\n",
    "# print(mnb_tfidf_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "empty-highlight",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4192  815]\n",
      " [ 617 4376]]\n"
     ]
    }
   ],
   "source": [
    "#confusion matrix for bag of words\n",
    "cm_bow=confusion_matrix(test_sentiments,mnb_bow_predict,labels=[1,0])\n",
    "print(cm_bow)\n",
    "# #confusion matrix for tfidf features\n",
    "# cm_tfidf=confusion_matrix(test_sentiments,mnb_tfidf_predict,labels=[1,0])\n",
    "# print(cm_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "revolutionary-comment",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
