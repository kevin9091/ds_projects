{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import product\n",
    "from collections import OrderedDict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.BN1 = nn.BatchNorm2d(16) \n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.BN2 = nn.BatchNorm1d(120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.pool(self.conv1(x)))\n",
    "        x = self.BN1(F.relu(self.pool(self.conv2(x))))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = self.BN2(F.relu(self.fc1(x)))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
    ")\n",
    "\n",
    "trainset = datasets.CIFAR10(root='../../../data/cifar-data', train=True, download=False, transform=transform)\n",
    "testset = datasets.CIFAR10(root='../../../data/cifar-data', train=False, download=False, transform=transform)\n",
    "trainsubset = torch.utils.data.Subset(trainset, np.random.choice(len(trainset), 9, replace=False))\n",
    "testsubset = torch.utils.data.Subset(testset, np.random.choice(len(testset), 9, replace=False))\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def imshow(img):\n",
    "#     img = img / 2 + 0.5     # unnormalize\n",
    "#     npimg = img.numpy()\n",
    "#     plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "#     plt.show()\n",
    "\n",
    "# # get some random training images\n",
    "# dataiter = iter(trainloader)\n",
    "# images, labels = dataiter.next()\n",
    "\n",
    "# # show images\n",
    "# imshow(torchvision.utils.make_grid(images))\n",
    "# # print labels\n",
    "# print(' '.join('%5s' % classes[labels[j]] for j in range(4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[100], [0.01]]\n"
     ]
    }
   ],
   "source": [
    "### Hyperparameters\n",
    "\n",
    "shuffle=True\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# optims = {'Momentum_SGD': optim.SGD(net.parameters(), lr=lr), \n",
    "#              'ADAM': optim.Adam(net.parameters(), lr=lr)}\n",
    "\n",
    "parameters = OrderedDict(\n",
    "batch_size=[100],\n",
    "lr = [0.01],\n",
    "# optimiser = list(optims.keys())\n",
    ")\n",
    "\n",
    "param_values = [v for v in parameters.values()]\n",
    "print(param_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cifar10 batch_size=100 lr=0.01\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.01\n",
      "    weight_decay: 0\n",
      ")\n",
      "[1,   100] loss: 1.741\n",
      "[1,   200] loss: 1.506\n",
      "[1,   300] loss: 1.396\n",
      "[1,   400] loss: 1.338\n",
      "[1,   500] loss: 1.261\n",
      "[2,   100] loss: 1.192\n",
      "[2,   200] loss: 1.181\n",
      "[2,   300] loss: 1.191\n",
      "[2,   400] loss: 1.142\n",
      "[2,   500] loss: 1.139\n",
      "At the end of epoch 2\n",
      "Accuracy of the network on the 50000 train images: 62.72 %\n",
      "Training Confusion Matrix:\n",
      "[[3119  168  280  106  141   68   50  102  636  330]\n",
      " [ 114 3482   26   87   32   33   94   34  119  979]\n",
      " [ 360   48 2531  363  500  558  316  173   90   61]\n",
      " [  90   44  357 1906  311 1431  364  238  130  129]\n",
      " [ 175   24  549  250 2825  440  211  395   68   63]\n",
      " [  62   25  242  797  288 2949  110  366   78   83]\n",
      " [  36   30  274  553  412  215 3317   49   47   67]\n",
      " [  55   34  206  215  401  492   31 3401   44  121]\n",
      " [ 312  251   66  108   37   28   56   24 3872  246]\n",
      " [ 133  332   26  124   44   36   48  144  157 3956]]\n",
      "Accuracy of the network on the 50000 train images: 62.81 %\n",
      "Training Confusion Matrix:\n",
      "50000 50000 50000 31406\n",
      "[[3146  164  293  111  121   69   49  100  620  327]\n",
      " [ 121 3471   19   89   33   38   93   32  126  978]\n",
      " [ 366   45 2549  374  473  569  307  171   83   63]\n",
      " [  99   38  352 1904  299 1453  359  240  130  126]\n",
      " [ 175   26  554  242 2835  431  230  388   59   60]\n",
      " [  66   21  235  810  292 2960  111  354   68   83]\n",
      " [  37   30  264  555  428  225 3302   46   43   70]\n",
      " [  54   34  209  207  411  504   30 3387   34  130]\n",
      " [ 310  250   60  100   43   24   58   20 3889  246]\n",
      " [ 135  327   19  136   46   39   48  137  150 3963]]\n",
      "Accuracy of the network on the 10000 test images: 59 %\n",
      "Validation Confusion Matrix:\n",
      "10000 10000\n",
      "[[599  44  71  20  29  11  12  11 141  62]\n",
      " [ 26 655   5  25   7   4  21   8  25 224]\n",
      " [ 75   9 452  83  99 127  70  50  19  16]\n",
      " [ 20   8  78 385  77 252  70  46  27  37]\n",
      " [ 32   4 109  57 515  98  50 109  18   8]\n",
      " [ 22   3  55 138  56 583  20  85  24  14]\n",
      " [  5   8  57 121  91  53 635  14   8   8]\n",
      " [ 15   5  38  52  82 105   8 662   6  27]\n",
      " [ 80  57  15  20   9   9  11   6 741  52]\n",
      " [ 29  80   9  18  11  14  13  38  26 762]]\n",
      "Accuracy of the network on the 10000 test images: 59.89 %\n",
      "Validation Confusion Matrix:\n",
      "10000 10000 10000 5989\n",
      "[[599  44  71  20  29  11  12  11 141  62]\n",
      " [ 26 655   5  25   7   4  21   8  25 224]\n",
      " [ 75   9 452  83  99 127  70  50  19  16]\n",
      " [ 20   8  78 385  77 252  70  46  27  37]\n",
      " [ 32   4 109  57 515  98  50 109  18   8]\n",
      " [ 22   3  55 138  56 583  20  85  24  14]\n",
      " [  5   8  57 121  91  53 635  14   8   8]\n",
      " [ 15   5  38  52  82 105   8 662   6  27]\n",
      " [ 80  57  15  20   9   9  11   6 741  52]\n",
      " [ 29  80   9  18  11  14  13  38  26 762]]\n",
      "Finished Training\n",
      "Wall time: 1min 34s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for batch_size, lr in product(*param_values):\n",
    "    trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=shuffle, num_workers=2)\n",
    "    testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "    net = Net().to(device)\n",
    "    optimizer = optim.Adam(net.parameters(), lr=lr)\n",
    "#     optimizer = optims[optimiser]\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    comment = f'cifar10 batch_size={batch_size} lr={lr}'\n",
    "    print(comment)\n",
    "    print(optimizer)\n",
    "#     tb = SummaryWriter(comment=comment)\n",
    "#     tb_count=0\n",
    "\n",
    "    for epoch in range(2): \n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "#             print(i,running_loss)\n",
    "            if i % 100 == 99:   \n",
    "#                 tb_count += 1\n",
    "#                 tb.add_scalar('Running Loss', running_loss/100, tb_count)\n",
    "                print('[%d, %5d] loss: %.3f' %(epoch + 1, i + 1, running_loss / 100))\n",
    "                running_loss = 0.0\n",
    "\n",
    "        if epoch % 2 == 1:\n",
    "            print('At the end of epoch %d' %(epoch+1))\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            with torch.no_grad():\n",
    "                preds=[]\n",
    "                targets=[]\n",
    "                for data in trainloader:\n",
    "                    images, labels = data[0].to(device), data[1].to(device)\n",
    "                    outputs = net(images)\n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "#                     print(predicted, labels)\n",
    "                    preds += list(predicted.cpu().detach().numpy().squeeze())\n",
    "                    targets += list(labels.cpu().detach().numpy().squeeze())\n",
    "                    total += labels.size(0)\n",
    "                    correct += (predicted == labels).sum().item()\n",
    "#             tb.add_scalar('Train Accuracy', 100 * correct / total, epoch+1)\n",
    "            print('Accuracy of the network on the 50000 train images: %.2f %%' % (100 * correct / total))\n",
    "            print('Training Confusion Matrix:')\n",
    "            print(confusion_matrix(targets, preds))\n",
    "\n",
    "            preds=[]\n",
    "            targets=[]\n",
    "            with torch.no_grad():\n",
    "                for data in trainloader:\n",
    "                    images, labels = data[0].to(device), data[1].to(device)\n",
    "                    outputs = net(images)\n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "#                     print(predicted, labels)\n",
    "                    preds += list(predicted.cpu().detach().numpy().squeeze())\n",
    "                    targets += list(labels.cpu().detach().numpy().squeeze())\n",
    "                    corrects = [i==j for (i,j) in zip(preds,targets)]\n",
    "            print('Accuracy of the network on the 50000 train images: %.2f %%' % (100 * sum(corrects) / len(targets)))\n",
    "            print('Training Confusion Matrix:')\n",
    "            print(len(targets), len(preds), len(corrects), sum(corrects))\n",
    "            print(confusion_matrix(targets, preds))\n",
    "\n",
    "            correct1 = 0\n",
    "            total1 = 0\n",
    "            with torch.no_grad():\n",
    "                preds=[]\n",
    "                targets=[]\n",
    "                for data in testloader:\n",
    "                    images, labels = data[0].to(device), data[1].to(device)\n",
    "                    outputs = net(images)\n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "#                     print(predicted, labels)\n",
    "                    preds += list(predicted.cpu().detach().numpy().squeeze())\n",
    "                    targets += list(labels.cpu().detach().numpy().squeeze())\n",
    "                    total1 += labels.size(0)\n",
    "                    correct1 += (predicted == labels).sum().item()\n",
    "#             tb.add_scalar('Test Accuracy', 100 * correct / total, epoch+1)\n",
    "            print('Accuracy of the network on the 10000 test images: %d %%' % (100 * correct1 / total1))\n",
    "            print('Validation Confusion Matrix:')\n",
    "            print(len(targets), len(preds))\n",
    "            print(confusion_matrix(targets, preds))\n",
    "\n",
    "            preds=[]\n",
    "            targets=[]\n",
    "            for data in testloader:\n",
    "                images, labels = data[0].to(device), data[1].to(device)\n",
    "                outputs = net(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "#                 print(predicted, labels)\n",
    "                preds += list(predicted.cpu().detach().numpy().squeeze())\n",
    "                targets += list(labels.cpu().detach().numpy().squeeze())\n",
    "                corrects = [i==j for (i,j) in zip(preds,targets)]\n",
    "            print('Accuracy of the network on the 10000 test images: %.2f %%' % (100 * sum(corrects) / len(targets)))\n",
    "            print('Validation Confusion Matrix:')\n",
    "            print(len(targets), len(preds), len(corrects), sum(corrects))\n",
    "            print(confusion_matrix(targets, preds))\n",
    "\n",
    "#     tb.close()\n",
    "    print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# correct = 0\n",
    "# total = 0\n",
    "# with torch.no_grad():\n",
    "#     for data in trainloader:\n",
    "#         images, labels = data[0].to(device), data[1].to(device)\n",
    "#         outputs = net(images)\n",
    "#         _, predicted = torch.max(outputs.data, 1)\n",
    "#         total += labels.size(0)\n",
    "#         correct += (predicted == labels).sum().item()\n",
    "\n",
    "# print('Accuracy of the network on the 60000 train images: %d %%' % (\n",
    "#     100 * correct / total))\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     for data in testloader:\n",
    "#         images, labels = data[0].to(device), data[1].to(device)\n",
    "#         outputs = net(images)\n",
    "#         _, predicted = torch.max(outputs.data, 1)\n",
    "#         total += labels.size(0)\n",
    "#         correct += (predicted == labels).sum().item()\n",
    "\n",
    "# print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "#     100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class_correct = list(0. for i in range(10))\n",
    "# class_total = list(0. for i in range(10))\n",
    "# with torch.no_grad():\n",
    "#     for data in testloader:\n",
    "#         images, labels = data[0].to(device), data[1].to(device)\n",
    "#         outputs = net(images)\n",
    "#         _, predicted = torch.max(outputs, 1)\n",
    "#         c = (predicted == labels).squeeze()\n",
    "#         for i in range(100):\n",
    "#             label = labels[i]\n",
    "#             class_correct[label] += c[i].item()\n",
    "#             class_total[label] += 1\n",
    "\n",
    "\n",
    "# for i in range(10):\n",
    "#     print('Accuracy of %5s : %2d %%' % (\n",
    "#         classes[i], 100 * class_correct[i] / class_total[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATH = './cifar_net.pth'\n",
    "# torch.save(net.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# net = Net()\n",
    "# net.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataiter = iter(testloader)\n",
    "# images, labels = dataiter.next()\n",
    "\n",
    "# # print images\n",
    "# imshow(torchvision.utils.make_grid(images[:4]))\n",
    "# print('GroundTruth: ', ' '.join('%5s' % classes[labels[j]] for j in range(4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outputs = net(images)\n",
    "# _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "# print('Predicted: ', ' '.join('%5s' % classes[predicted[j]]\n",
    "#                               for j in range(4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 10.21 %\n",
      "Validation Confusion Matrix:\n",
      "10000 10000 10000 1021\n",
      "[[ 93 129  69  60 123 146 100 102  92  86]\n",
      " [ 95 118  94  58  99 141  98  91 115  91]\n",
      " [ 83 116  86  60 115 126 101 107 118  88]\n",
      " [ 97 112  94  52 119 145  99  92 100  90]\n",
      " [ 87 141  70  55 116 131 100 114  92  94]\n",
      " [ 96 111  91  49 118 143  90 111 109  82]\n",
      " [ 90 121  74  42 108 156 111 101  94 103]\n",
      " [ 87 133  74  57  97 131 107 102 119  93]\n",
      " [ 99 120  93  52 120 137  87  98  97  97]\n",
      " [ 86 123  63  56 130 142 106  91 100 103]]\n",
      "Wall time: 2.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "preds=[]\n",
    "targets=[]\n",
    "for data in testloader:\n",
    "    images, labels = data[0].to(device), data[1].to(device)\n",
    "    outputs = net(images)\n",
    "    preds += list(predicted.cpu().detach().numpy().squeeze())\n",
    "    targets += list(labels.cpu().detach().numpy().squeeze())\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    corrects = [i==j for (i,j) in zip(preds,targets)]\n",
    "print('Accuracy of the network on the 10000 test images: %.2f %%' % (100 * sum(corrects) / len(targets)))\n",
    "print('Validation Confusion Matrix:')\n",
    "print(len(targets), len(preds), len(corrects), sum(corrects))\n",
    "print(confusion_matrix(targets, preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=[1,2]\n",
    "b=[1,3]\n",
    "c=[i==j for (i,j) in zip(a,b)]\n",
    "sum(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 32, 32, 3)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset.data[:100,:,:,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
